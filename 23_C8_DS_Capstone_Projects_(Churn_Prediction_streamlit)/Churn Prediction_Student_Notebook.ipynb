{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"Churn Prediction_Student_Notebook.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"Mg_ZAS0B2slE"},"source":["___\n","\n","<p style=\"text-align: center;\"><img src=\"https://docs.google.com/uc?id=1lY0Uj5R04yMY3-ZppPWxqCr5pvBLYPnV\" class=\"img-fluid\" alt=\"CLRSWY\"></p>\n","\n","___"]},{"cell_type":"markdown","metadata":{"id":"8EjVhtzq2slH"},"source":["# WELCOME!"]},{"cell_type":"markdown","metadata":{"id":"JqV3cXW-2slL"},"source":["Welcome to \"***Employee Churn Analysis Project***\". This is the second project of Capstone Project Series, which you will be able to build your own classification models for a variety of business settings. \n","\n","Also you will learn what is Employee Churn?, How it is different from customer churn, Exploratory data analysis and visualization of employee churn dataset using ***matplotlib*** and ***seaborn***, model building and evaluation using python ***scikit-learn*** package. \n","\n","You will be able to implement classification techniques in Python. Using Scikit-Learn allowing you to successfully make predictions with the Random Forest, Gradient Descent Boosting , KNN algorithms.\n","\n","At the end of the project, you will have the opportunity to deploy your model using *Streamlit*.\n","\n","Before diving into the project, please take a look at the determines and project structure.\n","\n","- NOTE: This project assumes that you already know the basics of coding in Python and are familiar with model deployement as well as the theory behind K-Means, Gradient Boosting , KNN, Random Forest, and Confusion Matrices. You can try more models and methods beside these to improve your model metrics.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"4oRnVXpS2slN"},"source":["# #Determines\n","In this project you have HR data of a company. A study is requested from you to predict which employee will churn by using this data.\n","\n","The HR dataset has 14,999 samples. In the given dataset, you have two types of employee one who stayed and another who left the company.\n","\n","You can describe 10 attributes in detail as:\n","- ***satisfaction_level:*** It is employee satisfaction point, which ranges from 0-1.\n","- ***last_evaluation:*** It is evaluated performance by the employer, which also ranges from 0-1.\n","- ***number_projects:*** How many of projects assigned to an employee?\n","- ***average_monthly_hours:*** How many hours in averega an employee worked in a month?\n","- **time_spent_company:** time_spent_company means employee experience. The number of years spent by an employee in the company.\n","- ***work_accident:*** Whether an employee has had a work accident or not.\n","- ***promotion_last_5years:*** Whether an employee has had a promotion in the last 5 years or not.\n","- ***Departments:*** Employee's working department/division.\n","- ***Salary:*** Salary level of the employee such as low, medium and high.\n","- ***left:*** Whether the employee has left the company or not.\n","\n","First of all, to observe the structure of the data, outliers, missing values and features that affect the target variable, you must use exploratory data analysis and data visualization techniques. \n","\n","Then, you must perform data pre-processing operations such as ***Scaling*** and ***Label Encoding*** to increase the accuracy score of Gradient Descent Based or Distance-Based algorithms. you are asked to perform ***Cluster Analysis*** based on the information you obtain during exploratory data analysis and data visualization processes. \n","\n","The purpose of clustering analysis is to cluster data with similar characteristics. You are asked to use the ***K-means*** algorithm to make cluster analysis. However, you must provide the K-means algorithm with information about the number of clusters it will make predictions. Also, the data you apply to the K-means algorithm must be scaled. In order to find the optimal number of clusters, you are asked to use the ***Elbow method***. Briefly, try to predict the set to which individuals are related by using K-means and evaluate the estimation results.\n","\n","Once the data is ready to be applied to the model, you must ***split the data into train and test***. Then build a model to predict whether employees will churn or not. Train your models with your train set, test the success of your model with your test set. \n","\n","Try to make your predictions by using the algorithms ***Gradient Boosting Classifier***, ***K Neighbors Classifier***, ***Random Forest Classifier***. You can use the related modules of the ***scikit-learn*** library. You can use scikit-learn ***Confusion Metrics*** module for accuracy calculation. You can use the ***Yellowbrick*** module for model selection and visualization.\n","\n","In the final step, you will deploy your model using Streamlit tool.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"97xzRLNj2slO"},"source":["# #Tasks\n","\n","#### 1. Exploratory Data Analysis\n","- Importing Modules\n","- Loading Dataset\n","- Data Insigts\n","\n","#### 2. Data Visualization\n","- Employees Left\n","- Determine Number of Projects\n","- Determine Time Spent in Company\n","- Subplots of Features\n","\n","#### 3. Data Pre-Processing\n","- Scaling\n","- Label Encoding\n","\n","#### 4. Cluster Analysis\n","- Find the optimal number of clusters (k) using the elbow method for for K-means.\n","- Determine the clusters by using K-Means then Evaluate predicted results.\n","\n","#### 5. Model Building\n","- Split Data as Train and Test set\n","- Built Gradient Boosting Classifier, Evaluate Model Performance and Predict Test Data\n","- Built K Neighbors Classifier and Evaluate Model Performance and Predict Test Data\n","- Built Random Forest Classifier and Evaluate Model Performance and Predict Test Data\n","\n","#### 6. Model Deployement\n","\n","- Save and Export the Model as .pkl\n","- Save and Export Variables as .pkl "]},{"cell_type":"markdown","metadata":{"id":"WLTGi7q02slP"},"source":["## 1. Exploratory Data Analysis\n","\n","Exploratory Data Analysis is an initial process of analysis, in which you can summarize characteristics of data such as pattern, trends, outliers, and hypothesis testing using descriptive statistics and visualization."]},{"cell_type":"code","metadata":{"id":"nyUCvXyU2slQ"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TyrWBiyM2sld"},"source":["### Importing Modules"]},{"cell_type":"code","metadata":{"id":"TI19sGjE2slf"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vS9n2J9-2sln"},"source":["### Loading Dataset\n","\n","Let's first load the required HR dataset using pandas's \"read_csv\" function."]},{"cell_type":"code","metadata":{"id":"rvS39ktq2slt"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wc8t0m9u2sl2"},"source":["### Data Insights\n","\n","In the given dataset, you have two types of employee one who stayed and another who left the company. So, you can divide data into two groups and compare their characteristics. Here, you can find the average of both the groups using groupby() and mean() function."]},{"cell_type":"code","metadata":{"id":"6Qd_Mxw-2sl9"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0PsO9Iew2smG"},"source":["## 2. Data Visualization\n","\n","You can search for answers to the following questions using data visualization methods. Based on these responses, you can develop comments about the factors that cause churn.\n","- How does the promotion status affect employee churn?\n","- How does years of experience affect employee churn?\n","- How does workload affect employee churn?\n","- How does the salary level affect employee churn?"]},{"cell_type":"markdown","metadata":{"id":"qRQhFwtq2smI"},"source":["### Employees Left\n","\n","Let's check how many employees were left?\n","Here, you can plot a bar graph using Matplotlib. The bar graph is suitable for showing discrete variable counts."]},{"cell_type":"code","metadata":{"id":"3aKWfFLk2smL"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vn6NHSZE2smY"},"source":["### Number of Projects\n","\n","Similarly, you can also plot a bar graph to count the number of employees deployed on how many projects?"]},{"cell_type":"code","metadata":{"id":"bGyyJcUP2sma"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"48X9SO4v2smj"},"source":["### Time Spent in Company\n","\n","Similarly, you can also plot a bar graph to count the number of employees have based on how much experience?\n"]},{"cell_type":"code","metadata":{"id":"OW-HRwfU2sml"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VEbtBv3q2smq"},"source":["### Subplots of Features\n","\n","You can use the methods of the matplotlib."]},{"cell_type":"code","metadata":{"id":"Gt8FWYQu2smu"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"36OyDJyx2sm2"},"source":["## 3. Data Pre-Processing"]},{"cell_type":"markdown","metadata":{"id":"iN94C5P42sm4"},"source":["#### Scaling\n","\n","Some machine learning algorithms are sensitive to feature scaling while others are virtually invariant to it. Machine learning algorithms like linear regression, logistic regression, neural network, etc. that use gradient descent as an optimization technique require data to be scaled. Also distance algorithms like KNN, K-means, and SVM are most affected by the range of features. This is because behind the scenes they are using distances between data points to determine their similarity.\n","\n","Scaling Types:\n","- Normalization: Normalization is a scaling technique in which values are shifted and rescaled so that they end up ranging between 0 and 1. It is also known as Min-Max scaling.\n","\n","- Standardization: Standardization is another scaling technique where the values are centered around the mean with a unit standard deviation. This means that the mean of the attribute becomes zero and the resultant distribution has a unit standard deviation.\n","\n","    "]},{"cell_type":"code","metadata":{"id":"1HXszRiq2sm4"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L8-SVBoq2snA"},"source":["#### Label Encoding\n","\n","Lots of machine learning algorithms require numerical input data, so you need to represent categorical columns in a numerical column. In order to encode this data, you could map each value to a number. e.g. Salary column's value can be represented as low:0, medium:1, and high:2. This process is known as label encoding, and sklearn conveniently will do this for you using LabelEncoder.\n","\n"]},{"cell_type":"code","metadata":{"id":"-pVP9UBQ2snC"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c1Gp2f7q2snF"},"source":["## 4. Cluster Analysis\n","\n","- Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters). It is a main task of exploratory data mining, and a common technique for statistical data analysis, used in many fields, including pattern recognition, image analysis, information retrieval, bioinformatics, data compression, computer graphics and machine learning.\n","\n","    [Cluster Analysis](https://en.wikipedia.org/wiki/Cluster_analysis)\n","\n","    [Cluster Analysis2](https://realpython.com/k-means-clustering-python/)"]},{"cell_type":"markdown","metadata":{"id":"TWQx_bhw2snG"},"source":["#### The Elbow Method\n","\n","- \"Elbow Method\" can be used to find the optimum number of clusters in cluster analysis. The elbow method is used to determine the optimal number of clusters in k-means clustering. The elbow method plots the value of the cost function produced by different values of k. If k increases, average distortion will decrease, each cluster will have fewer constituent instances, and the instances will be closer to their respective centroids. However, the improvements in average distortion will decline as k increases. The value of k at which improvement in distortion declines the most is called the elbow, at which we should stop dividing the data into further clusters.\n","\n","    [The Elbow Method](https://en.wikipedia.org/wiki/Elbow_method_(clustering)\n","\n","    [The Elbow Method2](https://medium.com/@mudgalvivek2911/machine-learning-clustering-elbow-method-4e8c2b404a5d)\n","\n","    [KMeans](https://towardsdatascience.com/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1)\n","\n","Let's find out the groups of employees who left. You can observe that the most important factor for any employee to stay or leave is satisfaction and performance in the company. So let's bunch them in the group of people using cluster analysis."]},{"cell_type":"code","metadata":{"id":"jMLD4mr32snH"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WpmbaABr2snN"},"source":["## 5. Model Building"]},{"cell_type":"markdown","metadata":{"id":"pYsKmaZd2snO"},"source":["### Split Data as Train and Test Set"]},{"cell_type":"markdown","metadata":{"id":"I6b_dTvA2snQ"},"source":["Here, Dataset is broken into two parts in ratio of 70:30. It means 70% data will used for model training and 30% for model testing."]},{"cell_type":"code","metadata":{"id":"S15Bpefl2snS"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y4d55Vek2snX"},"source":["### #Gradient Boosting Classifier"]},{"cell_type":"markdown","metadata":{"id":"L8OkbOrC2snY"},"source":["#### Model Building"]},{"cell_type":"code","metadata":{"id":"MefRCx542snY"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VAiUMdtI2snk"},"source":["#### Evaluating Model Performance"]},{"cell_type":"markdown","metadata":{"id":"92xg3rvR2snl"},"source":["- Confusion Matrix : You can use scikit-learn metrics module for accuracy calculation. A Confusion matrix is an N x N matrix used for evaluating the performance of a classification model, where N is the number of target classes. The matrix compares the actual target values with those predicted by the machine learning model. This gives us a holistic view of how well our classification model is performing and what kinds of errors it is making.\n","\n","    [Confusion Matrix](https://www.analyticsvidhya.com/blog/2020/04/confusion-matrix-machine-learning/)"]},{"cell_type":"markdown","metadata":{"id":"S9VeChm62snm"},"source":["- Yellowbrick: Yellowbrick is a suite of visualization and diagnostic tools that will enable quicker model selection. It’s a Python package that combines scikit-learn and matplotlib. Some of the more popular visualization tools include model selection, feature visualization, classification and regression visualization\n","\n","    [Yellowbrick](https://www.analyticsvidhya.com/blog/2018/05/yellowbrick-a-set-of-visualization-tools-to-accelerate-your-model-selection-process/)"]},{"cell_type":"code","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"id":"ber3WeUk2snn","outputId":"c634b756-d466-4a59-b083-468a5ce04495"},"source":["pip install yellowbrick"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: yellowbrick in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.2)\n","Requirement already satisfied: scikit-learn>=0.20 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from yellowbrick) (0.22.1)\n","Requirement already satisfied: scipy>=1.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from yellowbrick) (1.4.1)\n","Requirement already satisfied: numpy>=1.13.0 in c:\\users\\hp\\appdata\\roaming\\python\\python37\\site-packages (from yellowbrick) (1.19.0)\n","Requirement already satisfied: cycler>=0.10.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from yellowbrick) (0.10.0)\n","Requirement already satisfied: matplotlib!=3.0.0,>=2.0.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from yellowbrick) (3.1.3)\n","Requirement already satisfied: joblib>=0.11 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20->yellowbrick) (0.14.1)\n","Requirement already satisfied: six in c:\\users\\hp\\anaconda3\\lib\\site-packages (from cycler>=0.10.0->yellowbrick) (1.15.0)\n","Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (2.8.1)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (2.4.6)\n","Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (1.1.0)\n","Requirement already satisfied: setuptools in c:\\users\\hp\\anaconda3\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.2->yellowbrick) (50.3.0)\n","Note: you may need to restart the kernel to use updated packages.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CSUOz5302snx"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"90HfPd4w2sn1"},"source":["#### Prediction"]},{"cell_type":"markdown","metadata":{"id":"z9P157eX2sn2"},"source":["### #KNeighbors Classifier"]},{"cell_type":"markdown","metadata":{"id":"QPakx2ON2sn3"},"source":["#### Model Building"]},{"cell_type":"code","metadata":{"id":"HkNI16f72sn4"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MdGymWJ_2sn9"},"source":["#### Evaluating Model Performance"]},{"cell_type":"code","metadata":{"id":"TPF_wziW2soC"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_BYTdWlr2soJ"},"source":["#### Prediction"]},{"cell_type":"markdown","metadata":{"id":"PfMy1D_p2soK"},"source":["### #Random Forest Classifier"]},{"cell_type":"markdown","metadata":{"id":"M4GifMUw2soL"},"source":["#### Model Building"]},{"cell_type":"code","metadata":{"id":"uhjBZQbu2soN"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xRUPvrME2soc"},"source":["#### Evaluating Model Performance"]},{"cell_type":"code","metadata":{"id":"tXKuonpN2soe"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SKLtTwJ82som"},"source":["#### Prediction"]},{"cell_type":"code","metadata":{"id":"QdFUSrml2sop"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hv7E8XsazFMM"},"source":["## 6. Model Deployement\n","\n","You cooked the food in the kitchen and moved on to the serving stage. The question is how do you showcase your work to others? Model Deployement helps you showcase your work to the world and make better decisions with it. But, deploying a model can get a little tricky at times. Before deploying the model, many things such as data storage, preprocessing, model building and monitoring need to be studied. Streamlit is a popular open source framework used by data scientists for model distribution.\n","\n","Deployment of machine learning models, means making your models available to your other business systems. By deploying models, other systems can send data to them and get their predictions, which are in turn populated back into the company systems. Through machine learning model deployment, can begin to take full advantage of the model you built.\n","\n","Data science is concerned with how to build machine learning models, which algorithm is more predictive, how to design features, and what variables to use to make the models more accurate. However, how these models are actually used is often neglected. And yet this is the most important step in the machine learning pipline. Only when a model is fully integrated with the business systems, real values ​​can be extract from its predictions.\n","\n","After doing the following operations in this notebook, jump to new .py file and create your web app with Streamlit."]},{"cell_type":"markdown","metadata":{"id":"m5pwXBOkJPeM"},"source":["### Save and Export the Model as .pkl"]},{"cell_type":"code","metadata":{"id":"mmlin9CEzFr7"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r7sGSN6RJR6V"},"source":["### Save and Export Variables as .pkl"]},{"cell_type":"code","metadata":{"id":"2WeQNcROJScb"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aD6JV41czCKr"},"source":["___\n","\n","<p style=\"text-align: center;\"><img src=\"https://docs.google.com/uc?id=1lY0Uj5R04yMY3-ZppPWxqCr5pvBLYPnV\" class=\"img-fluid\" alt=\"CLRSWY\"></p>\n","\n","___"]}]}